<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ML/DL Core Concepts</title>
    <script type="text/javascript" id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f0f2f5;
            color: #333;
            line-height: 1.6;
        }
        .header {
            background-color: #2c3e50;
            color: #ecf0f1;
            padding: 20px 0;
            text-align: center;
            box-shadow: 0 2px 5px rgba(0,0,0,0.2);
        }
        .header h1 {
            margin: 0;
            font-size: 2.8em;
        }
        .navbar {
            background-color: #34495e;
            padding: 10px 0;
            text-align: center;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .navbar ul {
            list-style-type: none;
            margin: 0;
            padding: 0;
            display: inline-block; /* For centering */
        }
        .navbar li {
            display: inline-block;
            margin: 0 20px;
        }
        .navbar li a {
            color: #ecf0f1;
            text-decoration: none;
            font-weight: bold;
            font-size: 1.1em;
            transition: color 0.3s ease;
        }
        .navbar li a:hover {
            color: #1abc9c;
        }
        .content {
            max-width: 1000px;
            margin: 30px auto;
            padding: 30px;
            background-color: #ffffff;
            border-radius: 8px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }
        h2 {
            color: #2980b9;
            border-bottom: 2px solid #2980b9;
            padding-bottom: 8px;
            margin-top: 30px;
            font-size: 2em;
        }
        h3 {
            color: #27ae60;
            margin-top: 25px;
            font-size: 1.5em;
        }
        h4 {
            color: #e67e22;
            margin-top: 20px;
            font-size: 1.2em;
        }
        p {
            margin-bottom: 15px;
        }
        ul {
            list-style-type: disc;
            margin-left: 20px;
            margin-bottom: 15px;
        }
        ol {
            list-style-type: decimal;
            margin-left: 20px;
            margin-bottom: 15px;
        }
        li {
            margin-bottom: 8px;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        .code-block {
            background-color: #2d2d2d;
            color: #f8f8f2;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            margin-top: 15px;
            margin-bottom: 20px;
            font-family: 'Fira Code', 'Cascadia Code', 'Consolas', monospace;
            font-size: 0.9em;
        }
        .code-block pre {
            margin: 0;
            white-space: pre-wrap; /* Ensures code wraps */
        }
        .note {
            background-color: #e8f5e9;
            border-left: 5px solid #4CAF50;
            padding: 15px;
            margin-top: 20px;
            font-style: italic;
            color: #388e3c;
            border-radius: 4px;
        }
        .footer {
            text-align: center;
            padding: 20px;
            margin-top: 40px;
            background-color: #2c3e50;
            color: #ecf0f1;
            font-size: 0.9em;
            border-top: 1px solid #34495e;
        }
        .concept-section {
            margin-bottom: 40px;
            padding-bottom: 20px;
            border-bottom: 1px dashed #ccc;
        }
        .concept-section:last-of-type {
            border-bottom: none;
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>Core Concepts in Machine Learning & Deep Learning</h1>
        <p>A Comprehensive Guide</p>
    </div>

    <div class="navbar">
        <ul>
            <li><a href="AI.html" onclick="loadPage('AI.html'); return false;">AI</a></li>
            <li><a href="ML.html" onclick="loadPage('ML.html'); return false;">ML (Overview)</a></li>
            <li><a href="ML_Foundations.html" onclick="loadPage('ML_Foundations.html'); return false;">ML (Foundations)</a></li>
            <li><a href="ML_Intermediate_Algorithms.html" onclick="loadPage('ML_Intermediate_Algorithms.html'); return false;">ML (Intermediate)</a></li>
            <li><a href="ML_DL_Core_Concepts.html" onclick="loadPage('ML_DL_Core_Concepts.html'); return false;">ML/DL Core</a></li>
            <li><a href="NLP.html" onclick="loadPage('NLP.html'); return false;">NLP (Overview)</a></li>
            <li><a href="NLP_Foundations.html" onclick="loadPage('NLP_Foundations.html'); return false;">NLP (Foundations)</a></li>
        </ul>
    </div>

    <div class="content">
        <h2>Introduction to Foundational ML/DL Concepts</h2>
        <p>
            Machine Learning and Deep Learning, while powerful, are built upon a set of fundamental concepts that govern how models learn, optimize, and generalize. Understanding these core ideas is essential for anyone looking to build, evaluate, or deploy effective AI systems. This document provides a detailed breakdown of these crucial concepts, explaining their purpose, usage, and variations.
        </p>

        <div class="concept-section">
            <h3>1. Gradient</h3>
            <h4>Concept Definition:</h4>
            <p>
                In machine learning, the <strong>gradient</strong> is a vector of partial derivatives of a function with respect to its variables (often model parameters or weights). It indicates the direction of the steepest ascent of the function. For optimization tasks, particularly minimizing a loss function, we move in the opposite direction of the gradient (steepest descent).
            </p>
            <h4>Why Used:</h4>
            <ul>
                <li>It's the core component of <strong>Gradient Descent</strong> algorithms, which are used to iteratively adjust model parameters to minimize the loss function.</li>
                <li>It quantifies how much a small change in each parameter would affect the loss, guiding the learning process.</li>
            </ul>
            <h4>When to Use:</h4>
            <ul>
                <li>Whenever you are training an iterative model that uses an optimization algorithm like Gradient Descent (e.g., neural networks, linear regression trained with GD, logistic regression).</li>
                <li>In backpropagation, where gradients are computed for each layer to update weights.</li>
            </ul>
            <h4>How to Use (Coding Instruction - One Liner):</h4>
            <div class="code-block"><pre><code class="language-python">
# PyTorch: Computes gradients of the loss with respect to all trainable parameters
loss.backward()

# TensorFlow/Keras (within a GradientTape context):
# with tf.GradientTape() as tape:
#    loss = model(inputs)
# gradients = tape.gradient(loss, model.trainable_variables)
            </code></pre></div>
            <h4>Different Types / Variations:</h4>
            <ul>
                <li><strong>Batch Gradient Descent (BGD):</strong> Computes the gradient using the entire training dataset.</li>
                <li><strong>Stochastic Gradient Descent (SGD):</strong> Computes the gradient using a single randomly selected training example per update.</li>
                <li><strong>Mini-Batch Gradient Descent (MBGD):</strong> Computes the gradient using a small, randomly selected subset (batch) of the training data. This is the most common variant in deep learning.</li>
                <li><strong>Gradient Clipping:</strong> A technique to prevent exploding gradients by scaling down gradients if their L2 norm exceeds a certain threshold.</li>
                <li><strong>Gradient Accumulation:</strong> Accumulating gradients over several mini-batches before performing a single weight update, effectively simulating a larger batch size without increasing memory usage.</li>
            </ul>
            <h4>When to Use What Type:</h4>
            <ul>
                <li><strong>BGD:</strong> Suitable for small datasets. Provides stable convergence but is very slow for large datasets.</li>
                <li><strong>SGD:</strong> Faster than BGD for large datasets, but the updates are noisy, leading to more oscillations during training. Can help escape local minima.</li>
                <li><strong>Mini-Batch GD:</strong> The most widely used approach. It balances the stability of BGD with the speed of SGD. Batch size is a hyperparameter to tune.</li>
                <li><strong>Gradient Clipping:</strong> Essential when training deep networks, especially RNNs/LSTMs, to prevent numerical instability.</li>
                <li><strong>Gradient Accumulation:</strong> When you need a larger effective batch size but are limited by GPU memory.</li>
            </ul>
            <h4>Coding Related to That (Detailed):</h4>
            <div class="code-block"><pre><code class="language-python">
import torch
import torch.nn as nn
import torch.optim as optim

# Example of a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.linear = nn.Linear(10, 1) # 10 input features, 1 output

    def forward(self, x):
        return self.linear(x)

model = SimpleModel()
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Simulate a training step with mini-batch
inputs = torch.randn(32, 10) # A batch of 32 samples, 10 features each
targets = torch.randn(32, 1)

# Forward pass
outputs = model(inputs)
loss = criterion(outputs, targets)

# Zero previous gradients
optimizer.zero_grad()

# Backward pass: compute gradients
loss.backward()

# Update model parameters using the computed gradients
optimizer.step()

print(f"Loss after one step: {loss.item()}")

# Example of gradient clipping (PyTorch)
# After loss.backward()
# nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Clip gradients to a max norm of 1.0

# Example of gradient accumulation (conceptual)
# accumulated_gradients = []
# for i, (inputs, targets) in enumerate(dataloader):
#     outputs = model(inputs)
#     loss = criterion(outputs, targets) / accumulation_steps # Normalize loss
#     loss.backward()
#     if (i + 1) % accumulation_steps == 0:
#         optimizer.step()
#         optimizer.zero_grad()
            </code></pre></div>
        </div>

        <div class="concept-section">
            <h3>2. Loss Function (Cost Function / Objective Function)</h3>
            <h4>Concept Definition:</h4>
            <p>
                A <strong>loss function</strong> (or cost function) quantifies the discrepancy between the predicted output of a model and the actual true value. It's a measure of how "bad" the model's prediction is. The goal of training a machine learning model is to find the set of parameters that minimizes this loss function.
            </p>
            <h4>Why Used:</h4>
            <ul>
                <li><strong>Guidance for Optimization:</strong> It provides a clear objective for the optimization algorithm (like Gradient Descent) to follow. The gradient of the loss function tells the optimizer in which direction and how much to adjust the model's parameters.</li>
                <li><strong>Evaluation Metric:</strong> During training, the loss is continuously monitored to assess the model's learning progress.</li>
                <li><strong>Problem Specificity:</strong> Different problems (regression, classification) require different loss functions that inherently align with the nature of the output.</li>
            </ul>
            <h4>When to Use:</h4>
            <ul>
                <li>In every supervised machine learning task where the model learns from labeled data.</li>
                <li>During the training phase of neural networks and other iterative models.</li>
            </ul>
            <h4>How to Use (Coding Instruction - One Liner):</h4>
            <div class="code-block"><pre><code class="language-python">
# PyTorch: Mean Squared Error for regression
criterion = nn.MSELoss()

# PyTorch: Cross Entropy for classification
criterion = nn.CrossEntropyLoss()

# TensorFlow/Keras: Mean Squared Error
# loss_fn = tf.keras.losses.MeanSquaredError()

# TensorFlow/Keras: Categorical Crossentropy
# loss_fn = tf.keras.losses.CategoricalCrossentropy()
            </code></pre></div>
            <h4>Different Types / Variations:</h4>
            <ul>
                <li><strong>For Regression (Continuous Targets):</strong>
                    <ul>
                        <li><strong>Mean Squared Error (MSE):</strong> Average of the squared differences between predictions and actual values. Penalizes larger errors more heavily.
                            $$ \text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 $$</li>
                        <li><strong>Mean Absolute Error (MAE) / L1 Loss:</strong> Average of the absolute differences. Less sensitive to outliers than MSE.
                            $$ \text{MAE} = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i| $$</li>
                        <li><strong>Huber Loss:</strong> A combination of MSE and MAE, robust to outliers by being quadratic for small errors and linear for large errors.</li>
                    </ul>
                </li>
                <li><strong>For Classification (Categorical Targets):</strong>
                    <ul>
                        <li><strong>Binary Cross-Entropy (BCE) / Log Loss:</strong> For binary classification (two classes). Measures the difference between two probability distributions.
                            $$ \text{BCE} = - \frac{1}{N} \sum_{i=1}^{N} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)] $$</li>
                        <li><strong>Categorical Cross-Entropy (CCE):</strong> For multi-class classification where labels are one-hot encoded.
                            $$ \text{CCE} = - \sum_{i=1}^{N} y_{i,c} \log(\hat{y}_{i,c}) $$ (summed over classes c for each sample i)</li>
                        <li><strong>Sparse Categorical Cross-Entropy:</strong> Similar to CCE, but for multi-class classification where labels are integer-encoded.</li>
                        <li><strong>Hinge Loss:</strong> Primarily used with Support Vector Machines (SVMs). Penalizes incorrect classifications that are also confident.</li>
                    </ul>
                </li>
            </ul>
            <h4>When to Use What Type:</h4>
            <ul>
                <li><strong>MSE:</strong> Default for regression, good when large errors are particularly undesirable.</li>
                <li><strong>MAE:</strong> Use when outliers are present and you want to be less sensitive to them.</li>
                <li><strong>BCE/CCE:</strong> Standard for classification problems, especially when models output probabilities. Choose based on whether your labels are one-hot encoded or integer-encoded.</li>
                <li><strong>Huber Loss:</strong> When you need robustness to outliers in regression but still want smooth gradients near the optimum.</li>
                <li><strong>Hinge Loss:</strong> Specifically designed for "max-margin" classifiers like SVMs.</li>
            </ul>
            <h4>Coding Related to That (Detailed):</h4>
            <div class="code-block"><pre><code class="language-python">
import torch
import torch.nn as nn
import numpy as np

# Regression Loss Example (MSE)
predictions_reg = torch.tensor([10.5, 12.0, 9.8])
targets_reg = torch.tensor([10.0, 11.5, 10.0])
mse_loss = nn.MSELoss()
loss_val_mse = mse_loss(predictions_reg, targets_reg)
print(f"MSE Loss: {loss_val_mse.item():.4f}")

# Classification Loss Example (Binary Cross-Entropy)
# Predictions should be probabilities (after sigmoid for binary)
predictions_bce = torch.tensor([0.9, 0.1, 0.6], dtype=torch.float32)
targets_bce = torch.tensor([1.0, 0.0, 1.0], dtype=torch.float32)
bce_loss = nn.BCELoss() # Expects probabilities directly
loss_val_bce = bce_loss(predictions_bce, targets_bce)
print(f"BCE Loss: {loss_val_bce.item():.4f}")

# Classification Loss Example (Cross-Entropy Loss - for multi-class, often with logits)
# PyTorch's CrossEntropyLoss combines LogSoftmax and NLLLoss
# Predictions are raw scores (logits), targets are class indices
predictions_ce = torch.tensor([[2.0, 0.5, 1.0], [0.1, 1.5, 0.2]]) # Logits for 2 samples, 3 classes
targets_ce = torch.tensor([0, 1]) # Class indices: Sample 0 is class 0, Sample 1 is class 1
ce_loss = nn.CrossEntropyLoss()
loss_val_ce = ce_loss(predictions_ce, targets_ce)
print(f"Cross-Entropy Loss: {loss_val_ce.item():.4f}")
            </code></pre></div>
        </div>

        <div class="concept-section">
            <h3>3. Learning Rate</h3>
            <h4>Concept Definition:</h4>
            <p>
                The <strong>learning rate ($\alpha$ or $\eta$)</strong> is a hyperparameter that determines the step size at each iteration while moving towards a minimum of the loss function. In simpler terms, it controls how much the model's parameters are adjusted with respect to the gradient of the loss function.
            </p>
            <h4>Why Used:</h4>
            <ul>
                <li><strong>Convergence Control:</strong> A well-chosen learning rate ensures that the optimization algorithm converges efficiently to a minimum.</li>
                <li><strong>Stability:</strong> Too high a learning rate can cause the optimization to overshoot the minimum and diverge (loss increases or oscillates wildly).</li>
                <li><strong>Speed:</strong> Too low a learning rate will make the training process very slow, requiring many iterations to converge.</li>
            </ul>
            <h4>When to Use:</h4>
            <ul>
                <li>Whenever you are using a gradient-based optimization algorithm (e.g., SGD, Adam, RMSprop) to train a model.</li>
                <li>It is one of the most crucial hyperparameters to tune for deep learning models.</li>
            </ul>
            <h4>How to Use (Coding Instruction - One Liner):</h4>
            <div class="code-block"><pre><code class="language-python">
# PyTorch:
optimizer = optim.SGD(model.parameters(), lr=0.01)

# TensorFlow/Keras:
# optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
            </code></pre></div>
            <h4>Different Types / Variations:</h4>
            <ul>
                <li><strong>Fixed Learning Rate:</strong> The learning rate remains constant throughout training.</li>
                <li><strong>Adaptive Learning Rates (Optimizers):</strong> Algorithms like Adam, RMSprop, Adagrad, Adadelta automatically adjust the learning rate for each parameter based on past gradients. They often perform better out-of-the-box.</li>
                <li><strong>Learning Rate Schedulers (Decay Strategies):</strong> Algorithms that programmatically change the learning rate over time during training.
                    <ul>
                        <li><strong>Step Decay:</strong> Reduces the learning rate by a factor after a fixed number of epochs.</li>
                        <li><strong>Exponential Decay:</strong> Learning rate decays exponentially over time.</li>
                        <li><strong>Polynomial Decay:</strong> Learning rate decays polynomially over time.</li>
                        <li><strong>Cosine Annealing:</strong> Learning rate decays according to a cosine function, often returning to a higher value periodically (warm restarts).</li>
                        <li><strong>ReduceLROnPlateau:</strong> Reduces learning rate when a metric (e.g., validation loss) stops improving.</li>
                        <li><strong>Warm-up:</strong> Starts with a very low learning rate and gradually increases it during the initial epochs to allow the model to stabilize.</li>
                    </ul>
                </li>
            </ul>
            <h4>When to Use What Type:</h4>
            <ul>
                <li><strong>Fixed LR:</strong> Simple to implement, but requires careful manual tuning. Only recommended for very simple models or when fine-tuning specific tasks.</li>
                <li><strong>Adaptive Optimizers (Adam, RMSprop):</strong> Generally recommended for most deep learning tasks, as they require less manual tuning and often converge faster and more robustly. Adam is a good default choice.</li>
                <li><strong>Learning Rate Schedulers:</strong> Crucial for training very deep models or achieving state-of-the-art results.
                    <ul>
                        <li><strong>Step/Exponential/Polynomial Decay:</strong> Simple to implement and effective for long training runs.</li>
                        <li><strong>ReduceLROnPlateau:</strong> Useful when you want to continue training until convergence, but don't want to specify fixed decay points.</li>
                        <li><strong>Warm-up:</strong> Particularly beneficial for training large transformer models or when using very large batch sizes, preventing early instability.</li>
                    </ul>
                </li>
            </ul>
            <h4>Coding Related to That (Detailed):</h4>
            <div class="code-block"><pre><code class="language-python">
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(10, 1)
    def forward(self, x):
        return self.linear(x)

model = MyModel()
optimizer = optim.Adam(model.parameters(), lr=0.001) # Initial learning rate

# --- Example 1: Step Decay Learning Rate Scheduler (PyTorch) ---
# Reduces the LR by gamma=0.1 every step_size=10 epochs
scheduler_step = StepLR(optimizer, step_size=10, gamma=0.1)

print("--- Step Decay LR Schedule ---")
for epoch in range(1, 25): # Simulate 25 epochs
    # Simulate a training step
    loss = torch.randn(1) # Placeholder for actual loss
    # loss.backward()
    optimizer.step()

    print(f"Epoch {epoch:2d}, Current LR: {optimizer.param_groups[0]['lr']:.6f}")
    scheduler_step.step() # Update LR at the end of epoch

# Reset optimizer for next example
optimizer = optim.Adam(model.parameters(), lr=0.01)

# --- Example 2: ReduceLROnPlateau (PyTorch) ---
# Reduces LR if the monitored metric (e.g., validation loss) stops improving
scheduler_plateau = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)

print("\n--- ReduceLROnPlateau LR Schedule ---")
# Simulate training with varying validation losses
validation_losses = [1.0, 0.95, 0.9, 0.88, 0.87, 0.86, 0.85, # Improving
                     0.85, 0.86, 0.87, 0.88, # Plateau for 4 epochs (patience 3 + 1 = 4)
                     0.87, 0.86, 0.85, # Improves again
                     0.85, 0.85, 0.85, 0.85, 0.85, # Another plateau
                     0.7] # Finally improves significantly

for i, val_loss in enumerate(validation_losses):
    # Simulate a training step and then validate
    # loss.backward()
    optimizer.step()
    print(f"Epoch {i+1:2d}, Val Loss: {val_loss:.3f}, Current LR: {optimizer.param_groups[0]['lr']:.6f}")
    scheduler_plateau.step(val_loss) # Pass validation loss to the scheduler
            </code></pre></div>
        </div>

        <div class="concept-section">
            <h3>4. Temperature (in ML/DL)</h3>
            <h4>Concept Definition:</h4>
            <p>
                In machine learning, "temperature" is a hyperparameter often applied to the logits (raw prediction scores) before a softmax function, or used in sampling strategies for exploration. It's a scaling factor that controls the "peakiness" or "softness" of a probability distribution.
            </p>
            <h4>Why Used:</h4>
            <ul>
                <li><strong>Softmax Temperature Scaling:</strong> In classification or sequence generation (e.g., language models), temperature can smooth or sharpen the output probability distribution from a softmax function:
                    <ul>
                        <li><strong>High Temperature (>1):</strong> Makes the distribution softer, increasing the probability of less likely classes/tokens. Promotes diversity/exploration.</li>
                        <li><strong>Low Temperature (<1):</strong> Makes the distribution sharper, concentrating probability mass on the most likely classes/tokens. Promotes determinism/exploitation.</li>
                        <li><strong>Temperature = 1:</strong> Standard softmax.</li>
                    </ul>
                </li>
                <li><strong>Exploration-Exploitation Trade-off (Reinforcement Learning):</strong> In RL, temperature can control the degree of exploration in action selection, particularly in algorithms using softmax-like policies (e.g., Policy Gradients).</li>
            </ul>
            <h4>When to Use:</h4>
            <ul>
                <li>When generating text or other sequences from generative models (e.g., large language models) to control creativity vs. coherence.</li>
                <li>In reinforcement learning algorithms that use a probabilistic policy for action selection.</li>
                <li>In knowledge distillation, where a "teacher" model's soft probabilities are used to train a "student" model, often with a higher temperature to smooth the teacher's distribution.</li>
            </ul>
            <h4>How to Use (Coding Instruction - One Liner):</h4>
            <div class="code-block"><pre><code class="language-python">
# PyTorch: Applying temperature to logits before softmax
probabilities = torch.softmax(logits / temperature, dim=-1)

# TensorFlow:
# probabilities = tf.nn.softmax(logits / temperature)
            </code></pre></div>
            <h4>Different Types / Variations:</h4>
            <ul>
                <li><strong>Softmax Temperature:</strong> Applied directly to the logits before the softmax function, most common use.</li>
                <li><strong>RL Policy Temperature:</strong> Controls the exploration rate in reinforcement learning agents.</li>
                <li>While not "types," the common values are:
                    <ul>
                        <li><strong>T=1 (Default):</strong> Standard behavior.</li>
                        <li><strong>T &lt; 1 (e.g., 0.7):</strong> More confident, less diverse outputs.</li>
                        <li><strong>T > 1 (e.g., 1.5):</strong> More random, more diverse outputs.</li>
                    </ul>
                </li>
            </ul>
            <h4>When to Use What Type:</h4>
            <ul>
                <li><strong>Lower Temperature (e.g., 0.7-0.9):</strong> For tasks requiring coherent, deterministic, and factual generation (e.g., summarization, translation, code generation).</li>
                <li><strong>Higher Temperature (e.g., 1.0-1.5):</strong> For creative tasks like poetry generation, brainstorming, or when you want the model to explore less likely but potentially novel outputs. Be careful, too high can lead to gibberish.</li>
                <li><strong>Temperature in RL:</strong> To balance exploration (higher T for initial learning) and exploitation (lower T for refining policy).</li>
            </ul>
            <h4>Coding Related to That (Detailed):</h4>
            <div class="code-block"><pre><code class="language-python">
import torch
import torch.nn.functional as F
import numpy as np

# Simulate logits from a model for 5 classes
logits = torch.tensor([2.0, 1.0, 0.5, -0.5, -1.0])

print(f"Original Logits: {logits.numpy()}")

# --- Temperature = 1 (Standard Softmax) ---
temperature_1 = 1.0
probabilities_1 = F.softmax(logits / temperature_1, dim=0)
print(f"\nProbabilities (T={temperature_1}): {probabilities_1.numpy()}")
print(f"Sum: {probabilities_1.sum():.4f}")

# --- Lower Temperature (makes distribution sharper, more confident) ---
temperature_low = 0.5
probabilities_low = F.softmax(logits / temperature_low, dim=0)
print(f"\nProbabilities (T={temperature_low}): {probabilities_low.numpy()}")
print(f"Sum: {probabilities_low.sum():.4f}")

# --- Higher Temperature (makes distribution softer, more diverse) ---
temperature_high = 2.0
probabilities_high = F.softmax(logits / temperature_high, dim=0)
print(f"\nProbabilities (T={temperature_high}): {probabilities_high.numpy()}")
print(f"Sum: {probabilities_high.sum():.4f}")

# How this affects sampling (conceptual for text generation)
# If you sample from probabilities_low, you're very likely to pick the first class.
# If you sample from probabilities_high, you have a higher chance of picking other classes.
            </code></pre></div>
        </div>

        <div class="concept-section">
            <h3>5. Batch Size</h3>
            <h4>Concept Definition:</h4>
            <p>
                The <strong>batch size</strong> refers to the number of training examples utilized in one iteration. In a single forward/backward pass, the model processes a "batch" of data points, computes the average loss for that batch, and then updates the model parameters based on the gradients computed from that batch.
            </p>
            <h4>Why Used:</h4>
            <ul>
                <li><strong>Computational Efficiency:</strong> Processing data in batches allows for efficient use of computational resources (like GPUs) due to parallelization.</li>
                <li><strong>Gradient Estimation:</strong> Larger batch sizes provide a more accurate estimate of the true gradient of the loss function over the entire dataset, leading to more stable updates.</li>
                <li><strong>Regularization (Implicit):</strong> Smaller batch sizes introduce more noise into the gradient updates, which can sometimes act as a form of regularization, helping to escape shallow local minima and improve generalization.</li>
                <li><strong>Memory Constraints:</strong> Larger models and datasets might necessitate smaller batch sizes due to GPU memory limitations.</li>
            </ul>
            <h4>When to Use:</h4>
            <ul>
                <li>In virtually all deep learning training scenarios where datasets are large.</li>
                <li>When using mini-batch gradient descent or its variants.</li>
            </ul>
            <h4>How to Use (Coding Instruction - One Liner):</h4>
            <div class="code-block"><pre><code class="language-python">
# PyTorch: Creating a DataLoader with a specified batch size
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# TensorFlow/Keras: Specifying batch size in model.fit
# model.fit(train_dataset, batch_size=64, epochs=10)
            </code></pre></div>
            <h4>Different Types / Variations:</h4>
            <ul>
                <li><strong>Full Batch (Batch Size = N):</strong> The entire training dataset is used for one gradient update. Equivalent to Batch Gradient Descent.</li>
                <li><strong>Mini-Batch (Batch Size between 1 and N):</strong> A subset of the training data is used for each update. This is the most common approach. Typical batch sizes range from 16 to 256, but can go higher for very large models.</li>
                <li><strong>Stochastic (Batch Size = 1):</strong> Only one training example is used for each gradient update. Equivalent to Stochastic Gradient Descent.</li>
            </ul>
            <h4>When to Use What Type:</h4>
            <ul>
                <li><strong>Full Batch:</strong> Only practical for very small datasets. Guarantees true gradient direction, but very slow and memory-intensive for large data.</li>
                <li><strong>Mini-Batch:</strong> The default choice for most deep learning applications.
                    <ul>
                        <li><strong>Larger Batch Sizes (e.g., 128, 256, 512+):</strong> Can lead to faster training in terms of wall-clock time due to better GPU utilization. Gradients are more stable. May generalize slightly worse if not carefully tuned (can converge to sharper minima).</li>
                        <li><strong>Smaller Batch Sizes (e.g., 16, 32, 64):</strong> Can lead to better generalization (converge to flatter minima). Introduces more noise, which might help escape local minima. Slower in terms of wall-clock time per epoch due to more frequent updates.</li>
                    </ul>
                </li>
                <li><strong>Stochastic:</strong> Not commonly used in deep learning anymore due to poor computational efficiency and highly noisy updates. Can be useful in very specific online learning scenarios.</li>
            </ul>
            <h4>Coding Related to That (Detailed):</h4>
            <div class="code-block"><pre><code class="language-python">
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.optim as optim
import numpy as np

# 1. Define a custom dataset (simulated)
class CustomDataset(Dataset):
    def __init__(self, num_samples=1000, num_features=10):
        self.X = torch.randn(num_samples, num_features)
        self.y = torch.randn(num_samples, 1) # Dummy targets

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# 2. Instantiate dataset
full_dataset = CustomDataset(num_samples=1000)

# 3. Create DataLoaders with different batch sizes

# Mini-Batch Example
batch_size_mini = 64
mini_batch_loader = DataLoader(full_dataset, batch_size=batch_size_mini, shuffle=True)
print(f"Mini-Batch Loader with batch_size={batch_size_mini}:")
for i, (inputs, targets) in enumerate(mini_batch_loader):
    if i < 3: # Print info for first few batches
        print(f"  Batch {i+1}: inputs.shape={inputs.shape}, targets.shape={targets.shape}")
print(f"  Total mini-batches: {len(mini_batch_loader)}")

# Stochastic (Batch Size = 1) Example
batch_size_stochastic = 1
stochastic_loader = DataLoader(full_dataset, batch_size=batch_size_stochastic, shuffle=True)
print(f"\nStochastic Loader with batch_size={batch_size_stochastic}:")
for i, (inputs, targets) in enumerate(stochastic_loader):
    if i < 3:
        print(f"  Batch {i+1}: inputs.shape={inputs.shape}, targets.shape={targets.shape}")
print(f"  Total stochastic batches: {len(stochastic_loader)}")


# Full Batch Example (if dataset size allows)
# Note: DataLoader will create one batch equal to dataset size if batch_size >= len(dataset)
batch_size_full = len(full_dataset)
full_batch_loader = DataLoader(full_dataset, batch_size=batch_size_full, shuffle=True)
print(f"\nFull Batch Loader with batch_size={batch_size_full}:")
for i, (inputs, targets) in enumerate(full_batch_loader):
    print(f"  Batch {i+1}: inputs.shape={inputs.shape}, targets.shape={targets.shape}")
print(f"  Total full batches: {len(full_batch_loader)}")

# In a training loop:
# for epoch in range(num_epochs):
#     for inputs, targets in mini_batch_loader:
#         optimizer.zero_grad()
#         outputs = model(inputs)
#         loss = criterion(outputs, targets)
#         loss.backward()
#         optimizer.step()
            </code></pre></div>
        </div>

        <div class="concept-section">
            <h3>6. Epoch</h3>
            <h4>Concept Definition:</h4>
            <p>
                An <strong>epoch</strong> represents one complete pass through the entire training dataset during the training of a machine learning model. After one epoch, every single training example has been seen by the model once, and its parameters have been updated accordingly.
            </p>
            <h4>Why Used:</h4>
            <ul>
                <li><strong>Full Data Exposure:</strong> Ensures the model has the opportunity to learn from all available training data.</li>
                <li><strong>Learning Progress:</strong> Training typically spans multiple epochs, as a single pass is usually not enough for the model to fully learn complex patterns in the data.</li>
                <li><strong>Monitoring:</strong> Loss and accuracy are usually monitored per epoch to track training progress and detect issues like overfitting or underfitting.</li>
            </ul>
            <h4>When to Use:</h4>
            <ul>
                <li>In almost all iterative training processes for machine learning and deep learning models.</li>
                <li>As a primary unit to track the duration of training.</li>
            </ul>
            <h4>How to Use (Coding Instruction - One Liner):</h4>
            <div class="code-block"><pre><code class="language-python">
# Python loop for training epochs
for epoch in range(num_epochs):
    # Training code for one epoch goes here
            </code></pre></div>
            <h4>Different Types / Variations:</h4>
            <p>
                The concept of an "epoch" itself doesn't have different "types" in the same way as other hyperparameters. It's a fundamental unit of training duration. However, there are related concepts:
            </p>
            <ul>
                <li><strong>Number of Epochs:</strong> This is the hyperparameter you set. Too few can lead to underfitting; too many can lead to overfitting.</li>
                <li><strong>Early Stopping:</strong> A technique where training is stopped prematurely (before all planned epochs are complete) if the model's performance on a validation set stops improving, or starts degrading. This prevents overfitting.</li>
            </ul>
            <h4>When to Use What Type:</h4>
            <ul>
                <li>The appropriate number of epochs is determined by trial and error, cross-validation, and monitoring validation loss/metrics.</li>
                <li><strong>Early Stopping:</strong> Highly recommended for almost all deep learning projects. It's a robust way to prevent overfitting without having to manually guess the optimal number of epochs.</li>
            </ul>
            <h4>Coding Related to That (Detailed):</h4>
            <div class="code-block"><pre><code class="language-python">
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# --- Simulate a simple training setup ---
# Dummy data
X_train = torch.randn(1000, 10)
y_train = torch.randn(1000, 1)
train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# Simple model
class SimpleNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(10, 1)
    def forward(self, x):
        return self.linear(x)

model = SimpleNet()
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

num_epochs = 10 # Define the total number of epochs

print("--- Training Loop with Epochs ---")
for epoch in range(num_epochs):
    model.train() # Set model to training mode
    total_loss = 0
    for batch_idx, (inputs, targets) in enumerate(train_loader):
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss_epoch = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss_epoch:.4f}")

    # --- Early Stopping Concept (pseudo-code) ---
    # if validation_loss_for_this_epoch > best_validation_loss:
    #     patience_counter += 1
    # else:
    #     best_validation_loss = validation_loss_for_this_epoch
    #     patience_counter = 0
    # if patience_counter >= max_patience:
    #     print("Early stopping triggered.")
    #     break # Exit the training loop
            </code></pre></div>
        </div>

        <div class="concept-section">
            <h3>7. Attention Mechanism</h3>
            <h4>Concept Definition:</h4>
            <p>
                The <strong>Attention Mechanism</strong> allows a neural network to dynamically focus on specific parts of its input data that are most relevant for making a prediction or generating an output. Instead of processing all parts of the input equally, it assigns varying "weights" (attention scores) to different parts, allowing the model to highlight the most informative elements.
            </p>
            <h4>Why Used:</h4>
            <ul>
                <li><strong>Addresses Long-Range Dependencies:</strong> Solves the bottleneck problem of traditional RNNs/LSTMs that struggle with very long sequences by allowing direct access to distant parts of the input.</li>
                <li><strong>Improved Performance:</strong> Significantly boosts performance in sequence-to-sequence tasks (e.g., machine translation, text summarization) and has become a cornerstone of modern deep learning architectures.</li>
                <li><strong>Interpretability:</strong> The attention weights can often be visualized, providing insights into which parts of the input the model is focusing on, thus improving model interpretability.</li>
            </ul>
            <h4>When to Use:</h4>
            <ul>
                <li>Primarily in sequence models, especially those dealing with variable-length inputs and outputs (e.g., Natural Language Processing, speech recognition).</li>
                <li>Integral part of the Transformer architecture, which is dominant in modern NLP (BERT, GPT, T5) and increasingly in Computer Vision (Vision Transformers).</li>
            <li>Any scenario where certain parts of the input are more important than others for a given task.</li>
            </ul>
            <h4>How to Use (Coding Instruction - One Liner):</h4>
            <div class="code-block"><pre><code class="language-python">
# PyTorch: Multi-Head Attention module from Transformer
attention_output, _ = nn.MultiheadAttention(embed_dim, num_heads)(query, key, value)

# TensorFlow/Keras: MultiHeadAttention layer
# attention_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(query, value, key)
            </code></pre></div>
            <h4>Different Types / Variations:</h4>
            <p>Attention mechanisms have evolved significantly:</p>
            <ul>
                <li><strong>Encoder-Decoder Attention (Bahdanau/Luong Attention):</strong>
                    <ul>
                        <li><strong>Bahdanau (Additive) Attention:</strong> Introduced with Neural Machine Translation. Uses a feed-forward network to compute attention scores.</li>
                        <li><strong>Luong (Multiplicative) Attention:</strong> A simpler dot-product based attention. Also common in seq2seq models.</li>
                        <li><em>When to Use:</em> In classic encoder-decoder architectures where the decoder needs to focus on relevant parts of the encoder's output (e.g., machine translation).</li>
                    </ul>
                </li>
                <li><strong>Self-Attention (Intra-Attention):</strong>
                    <ul>
                        <li>Each element in a sequence attends to all other elements in the *same* sequence to compute a new representation of itself.</li>
                        <li>The core mechanism of the Transformer architecture. Computes Query, Key, Value matrices from the input sequence.
                            $$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$
                            where $Q$ (Query), $K$ (Key), $V$ (Value) are linear transformations of the input embeddings, and $d_k$ is the dimension of the keys.</li>
                    </ul>
                </li>
                <li><strong>Multi-Head Attention:</strong>
                    <ul>
                        <li>An extension of self-attention where the attention mechanism is run multiple times in parallel ("heads"). Each head learns to focus on different aspects of the input. Their outputs are then concatenated and linearly transformed.</li>
                        <li><em>When to Use:</em> Default in Transformers. Allows the model to capture diverse relationships and dependencies within the data.</li>
                    </ul>
                </li>
                <li><strong>Cross-Attention:</strong>
                    <ul>
                        <li>Similar to encoder-decoder attention but used more generally within Transformer blocks (e.g., in the decoder, where Queries come from the decoder's output, and Keys/Values come from the encoder's output).</li>
                        <li><em>When to Use:</em> When one sequence needs to attend to another (e.g., text to image, where text query attends to image features).</li>
                    </ul>
                </li>
            </ul>
            <h4>When to Use What Type:</h4>
            <ul>
                <li><strong>Self-Attention / Multi-Head Attention:</strong> Standard for modern sequence processing tasks (NLP, even some CV), forming the backbone of Transformer models like BERT, GPT, T5.</li>
                <li><strong>Encoder-Decoder Attention (Bahdanau/Luong):</strong> If you're building a traditional sequence-to-sequence model with separate encoder and decoder RNNs/LSTMs, these are the choices.</li>
                <li>The specific type is often dictated by the chosen architecture (e.g., if you use a Transformer, you use Multi-Head Self-Attention and Cross-Attention).</li>
            </ul>
            <h4>Coding Related to That (Detailed):</h4>
            <div class="code-block"><pre><code class="language-python">
import torch
import torch.nn as nn
import torch.nn.functional as F

# --- Conceptual Self-Attention Implementation (Simplified) ---
# This is a highly simplified version to illustrate the matrix multiplications
# It omits multi-head, masking, and proper scaling/biases for brevity.

class SimpleSelfAttention(nn.Module):
    def __init__(self, embed_dim):
        super().__init__()
        self.embed_dim = embed_dim
        # Learnable weight matrices for Query, Key, Value
        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)
        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)

    def forward(self, x):
        # x is assumed to be (batch_size, sequence_length, embed_dim)
        Q = self.W_q(x) # Query matrix
        K = self.W_k(x) # Key matrix
        V = self.W_v(x) # Value matrix

        # Calculate attention scores (dot product attention)
        # (batch_size, seq_len, embed_dim) @ (batch_size, embed_dim, seq_len) -> (batch_size, seq_len, seq_len)
        attention_scores = torch.matmul(Q, K.transpose(-2, -1))

        # Scale the scores
        attention_scores = attention_scores / (self.embed_dim ** 0.5)

        # Apply softmax to get attention weights
        attention_weights = F.softmax(attention_scores, dim=-1)

        # Apply weights to values to get context vector
        # (batch_size, seq_len, seq_len) @ (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)
        output = torch.matmul(attention_weights, V)
        return output, attention_weights # Also return weights for visualization

# Example Usage:
embed_dim = 64
seq_len = 5
batch_size = 2

# Simulate input embeddings
input_embeddings = torch.randn(batch_size, seq_len, embed_dim)

simple_attention = SimpleSelfAttention(embed_dim)
output_features, attention_map = simple_attention(input_embeddings)

print("\n--- Simple Self-Attention (Conceptual) ---")
print(f"Input Embeddings Shape: {input_embeddings.shape}")
print(f"Output Features Shape (after attention): {output_features.shape}")
print(f"Attention Map Shape (how each token attends to others): {attention_map.shape}")
# Example: attention_map[0] shows the attention weights for the first sequence in the batch
# attention_map[0][0] is the attention weights of the first token on all other tokens

# --- Using PyTorch's built-in MultiheadAttention (more practical) ---
# Note: For MultiheadAttention, query, key, value are typically
# (sequence_length, batch_size, embed_dim) not (batch_size, sequence_length, embed_dim)
# We need to permute dimensions.

num_heads = 8
embed_dim_mha = 256 # Must be divisible by num_heads

mha = nn.MultiheadAttention(embed_dim=embed_dim_mha, num_heads=num_heads, batch_first=False) # batch_first=False is default

# Simulate input for MHA (seq_len, batch_size, embed_dim)
query = torch.randn(seq_len, batch_size, embed_dim_mha)
key = torch.randn(seq_len, batch_size, embed_dim_mha)
value = torch.randn(seq_len, batch_size, embed_dim_mha)

attn_output, attn_output_weights = mha(query, key, value)

print("\n--- PyTorch MultiheadAttention ---")
print(f"Query/Key/Value Shape: {query.shape}")
print(f"Attention Output Shape: {attn_output.shape}") # Same shape as query
print(f"Attention Weights Shape: {attn_output_weights.shape}") # (batch_size * num_heads, seq_len, seq_len) if batch_first=False or (batch_size, seq_len, seq_len) if average_attn_weights=True
            </code></pre></div>
        </div>

        <div class="concept-section">
            <h3>8. Tokens</h3>
            <h4>Concept Definition:</h4>
            <p>
                In Natural Language Processing (NLP), <strong>tokens</strong> are the fundamental units into which a text sequence is broken down for processing by a model. This process is called tokenization. Tokens can represent words, subword units, or even individual characters.
            </p>
            <h4>Why Used:</h4>
            <ul>
                <li><strong>Numerical Representation:</strong> Converts unstructured raw text into a discrete, numerical format that machine learning models can understand and process.</li>
                <li><strong>Vocabulary Management:</strong> Maps text units to unique integer IDs, allowing models to learn embeddings for these units.</li>
                <li><strong>Handling Variability:</strong> Different tokenization strategies help manage vocabulary size and deal with out-of-vocabulary (OOV) words.</li>
            </ul>
            <h4>When to Use:</h4>
            <ul>
                <li>At the very beginning of any NLP pipeline, before feeding text data into models.</li>
                <li>Whenever working with text data for tasks like sentiment analysis, machine translation, text generation, question answering, etc.</li>
            </ul>
            <h4>How to Use (Coding Instruction - One Liner):</h4>
            <div class="code-block"><pre><code class="language-python">
# Using Hugging Face Transformers tokenizer
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
tokens = tokenizer.encode("Hello world, this is a test.", add_special_tokens=True)
            </code></pre></div>
            <h4>Different Types / Variations:</h4>
            <ul>
                <li><strong>Word-level Tokens:</strong> Splits text into individual words.
                    <ul>
                        <li><em>Pros:</em> Intuitive, human-readable.</li>
                        <li><em>Cons:</em> Very large vocabulary for large corpora, struggles with OOV words (e.g., new words, misspellings, proper nouns).</li>
                        <li><em>When to Use:</em> Simpler NLP tasks, or when working with small, controlled vocabularies.</li>
                    </ul>
                </li>
                <li><strong>Subword Tokens:</strong> Breaks down words into smaller, frequently occurring subword units. This is the most common approach in modern NLP.
                    <ul>
                        <li><strong>Byte-Pair Encoding (BPE):</strong> Learns a vocabulary of merges from pairs of characters or character sequences.
                            Example: "unpredictable" might become ["un", "predict", "able"].</li>
                        <li><strong>WordPiece:</strong> Used by BERT. Similar to BPE but prioritizes units that maximize the likelihood of a language model.</li>
                        <li><strong>SentencePiece:</strong> Used by Google's T5, XLNet. Treats the input as a raw stream of characters, including spaces, allowing for consistent tokenization regardless of input whitespace.</li>
                        <li><em>Pros:</em> Manages vocabulary size effectively (smaller than word-level, larger than char-level), handles OOV words by breaking them into known subwords, better for morphology.</li>
                        <li><em>Cons:</em> Less intuitive to read than whole words.</li>
                        <li><em>When to Use:</em> Standard for most modern large language models (BERT, GPT, T5, Llama).</li>
                    </ul>
                </li>
                <li><strong>Character-level Tokens:</strong> Splits text into individual characters.
                    <ul>
                        <li><em>Pros:</em> Smallest vocabulary, no OOV words (all characters are known).</li>
                        <li><em>Cons:</em> Longer sequences, loses semantic meaning of words, requires deeper models to learn word-level patterns.</li>
                        <li><em>When to Use:</em> Very rare languages, highly noisy text, or when working with very small models.</li>
                    </ul>
                </li>
            </ul>
            <h4>When to Use What Type:</h4>
            <ul>
                <li>For <strong>most modern NLP applications</strong>, especially with pre-trained models from Hugging Face or similar libraries, <strong>subword tokenization (BPE, WordPiece, SentencePiece)</strong> is the default and recommended choice. It offers the best balance between vocabulary size, handling of OOV words, and semantic representation.</li>
                <li><strong>Word-level</strong> can be used for simpler, legacy systems or when fine-grained control over full words is desired for analysis.</li>
                <li><strong>Character-level</strong> is niche, for highly specialized tasks, very low-resource languages, or certain types of noisy text processing.</li>
            </ul>
            <h4>Coding Related to That (Detailed):</h4>
            <div class="code-block"><pre><code class="language-python">
from transformers import AutoTokenizer

# --- 1. Subword Tokenization (most common with LLMs) ---
# Load a pre-trained tokenizer (e.g., BERT's WordPiece)
tokenizer_bert = AutoTokenizer.from_pretrained("bert-base-uncased")

text_subword = "This is an example sentence for subword tokenization, including a_new_word."
tokenized_subword = tokenizer_bert.tokenize(text_subword)
encoded_subword = tokenizer_bert.encode(text_subword, add_special_tokens=True) # Adds [CLS], [SEP]
decoded_subword = tokenizer_bert.decode(encoded_subword)

print("--- Subword Tokenization (BERT) ---")
print(f"Original Text: '{text_subword}'")
print(f"Tokenized: {tokenized_subword}") # Shows subword units (e.g., '##word')
print(f"Encoded (IDs with special tokens): {encoded_subword}")
print(f"Decoded: '{decoded_subword}'")


# --- 2. Character-level Tokenization (Conceptual, often custom) ---
# For actual char-level, you might build a simple tokenizer or use specialized libraries
def char_tokenize(text):
    return list(text)

text_char = "Hello World"
tokenized_char = char_tokenize(text_char)
print("\n--- Character-level Tokenization ---")
print(f"Original Text: '{text_char}'")
print(f"Tokenized: {tokenized_char}")

# --- 3. Word-level Tokenization (Conceptual, usually NLTK/SpaCy) ---
import nltk
from nltk.tokenize import word_tokenize
# nltk.download('punkt') # Uncomment to download if you don't have it

text_word = "Hello World. How are you?"
tokenized_word = word_tokenize(text_word)
print("\n--- Word-level Tokenization (NLTK) ---")
print(f"Original Text: '{text_word}'")
print(f"Tokenized: {tokenized_word}")
            </code></pre></div>
        </div>

        <div class="concept-section">
            <h3>9. Context Window (for LLMs)</h3>
            <h4>Concept Definition:</h4>
            <p>
                The <strong>context window</strong> (also known as context length or sequence length limit) of a Large Language Model (LLM) refers to the maximum number of tokens that the model can process and "attend to" at once to generate its next output token. This limit dictates how much information from the past conversation or document the model can effectively "remember" and utilize.
            </p>
            <h4>Why Used:</h4>
            <ul>
                <li><strong>Computational Constraints:</strong> Transformers, which are the backbone of LLMs, have a self-attention mechanism whose computational cost ($O(N^2)$ where $N$ is sequence length) increases quadratically with the context window size. This makes very long contexts computationally expensive.</li>
                <li><strong>Memory Constraints:</strong> Storing attention matrices and intermediate activations also consumes memory quadratically with context length.</li>
                <li><strong>Model Design:</strong> It's a fundamental design parameter that defines the "memory capacity" of the model.</li>
            </ul>
            <h4>When to Use:</h4>
            <ul>
                <li>When interacting with or designing applications using Large Language Models.</li>
                <li>When choosing an LLM for a specific task; the required context length for the task is a critical factor.</li>
                <li>When performing tasks that require understanding long-range dependencies, such as summarization of long documents, analyzing entire codebases, or extended conversations.</li>
            </ul>
            <h4>How to Use (Coding Instruction - One Liner):</h4>
            <div class="code-block"><pre><code class="language-python">
# Accessing max context length in Hugging Face Transformers model config
# model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")
# max_context_length = model.config.max_position_embeddings

# Specifying max length during text generation
# generated_text = model.generate(input_ids, max_length=1024)
            </code></pre></div>
            <h4>Different Types / Variations:</h4>
            <p>While the core concept is a fixed numerical limit, the way models handle or extend this can vary:</p>
            <ul>
                <li><strong>Fixed Context Window:</strong> The traditional approach where the model has a hard limit (e.g., 512, 1024, 2048, 4096, 8192, 16384, 32768, 128000 tokens) on the sequence length it can process. If input exceeds this, it's typically truncated.</li>
                <li><strong>Sliding Window Attention (Local Attention):</strong> Some models use a sliding window over the input sequence, where each token only attends to a fixed number of tokens around it, rather than the entire sequence. This reduces computational complexity to $O(N)$.
                    <ul>
                        <li><em>Examples:</em> Longformer, Reformer.</li>
                        <li><em>When to Use:</em> When dealing with very long documents where full quadratic attention is prohibitive, but some local context is sufficient.</li>
                    </ul>
                </li>
                <li><strong>Hierarchical Attention:</strong> Breaks down long documents into segments, processes segments, and then applies another layer of attention over the segment representations.
                    <ul>
                        <li><em>When to Use:</em> For extremely long documents (e.g., books) where hierarchical structure is natural.</li>
                    </ul>
                </li>
                <li><strong>"Long-Context" Models:</strong> Recent advancements (e.g., GPT-4 Turbo, Claude 2.1, Gemini 1.5 Pro) have significantly expanded context windows to hundreds of thousands or even millions of tokens through architectural innovations, memory optimizations, or new attention variants.</li>
            </ul>
            <h4>When to Use What Type:</h4>
            <ul>
                <li>For general LLM use cases and common applications, most modern models provide a <strong>fixed context window</strong> of sufficient size (e.g., 4k to 32k tokens) that is suitable.</li>
                <li>If you are working with extremely long documents (e.g., entire books, very long codebases) and need to maintain coherence over the *entire* document, you would specifically seek out <strong>"Long-Context" models</strong> or models that implement <strong>sliding window attention</strong> or <strong>hierarchical attention</strong> as part of their architecture.</li>
                <li>For specific research or custom model development, understanding these variations allows you to choose/design the attention mechanism most appropriate for your context length requirements and computational budget.</li>
            </ul>
            <h4>Coding Related to That (Detailed):</h4>
            <div class="code-block"><pre><code class="language-python">
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load a pre-trained model and tokenizer
model_name = "openai-community/gpt2" # GPT-2 small
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# --- 1. Accessing Max Context Window ---
max_context_length = model.config.max_position_embeddings
print(f"Max context window for {model_name}: {max_context_length} tokens")

# --- 2. Tokenizing and Handling Input Exceeding Context Window ---
long_text = "The quick brown fox jumps over the lazy dog. " * 50 # Create a long text
print(f"\nLength of long_text (characters): {len(long_text)}")

# Encode with truncation
# max_length: The maximum length (in tokens) for the inputs to be fed to the model.
# truncation=True: Ensures that sequences longer than max_length are truncated.
# return_tensors="pt": Returns PyTorch tensors.
inputs_truncated = tokenizer(long_text, max_length=max_context_length,
                             truncation=True, return_tensors="pt")

print(f"Input IDs shape (truncated to max context): {inputs_truncated['input_ids'].shape}")
print(f"Number of tokens (truncated): {inputs_truncated['input_ids'].shape[1]}")

# If you don't truncate, it will raise an error if input is too long for the model
# try:
#     tokenizer(long_text, return_tensors="pt")
# except ValueError as e:
#     print(f"\nError without truncation: {e}")

# --- 3. Generating text within a context window ---
prompt = "The capital of France is Paris. The language spoken there is"
input_ids = tokenizer.encode(prompt, return_tensors="pt")

# Generate text, ensuring max_length considers the prompt length + new tokens
# max_length specifies the total length of the generated sequence (prompt + new tokens)
generated_output = model.generate(input_ids, max_length=input_ids.shape[1] + 20,
                                  num_return_sequences=1, pad_token_id=tokenizer.eos_token_id) # pad_token_id to avoid warning

generated_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)
print(f"\n--- Text Generation Example ---")
print(f"Prompt: '{prompt}'")
print(f"Generated: '{generated_text}'")
print(f"Generated sequence length: {generated_output.shape[1]} tokens")

# Note: For models designed for very long contexts (e.g., Longformer, BigBird, Gemini 1.5 Pro),
# their internal attention mechanisms handle the scalability, but the user interaction
# with the tokenizer and model.generate() remains conceptually similar, just with much larger `max_length` values.
            </code></pre></div>
        </div>

        <div class="concept-section">
            <h3>10. Fine-tuning</h3>
            <h4>Concept Definition:</h4>
            <p>
                <strong>Fine-tuning</strong> is a transfer learning technique where a pre-trained model (trained on a very large, general dataset) is further trained on a smaller, task-specific dataset. The goal is to adapt the model's learned features and knowledge to perform well on a new, related downstream task.
            </p>
            <h4>Why Used:</h4>
            <ul>
                <li><strong>Leverage Pre-trained Knowledge:</strong> Avoids training a model from scratch, which is computationally expensive and requires vast amounts of data. Pre-trained models have already learned general representations (e.g., language understanding, image features).</li>
                <li><strong>Faster Convergence:</strong> Fine-tuning typically converges much faster than training from scratch.</li>
                <li><strong>Improved Performance:</strong> Often leads to significantly better performance on downstream tasks, especially when the task-specific dataset is small.</li>
                <li><strong>Resource Efficiency:</strong> Reduces the data and computational resources needed for new tasks.</li>
            </ul>
            <h4>When to Use:</h4>
            <ul>
                <li>When you have a specific task (e.g., text classification, object detection, question answering) for which you have a labeled dataset.</li>
                <li>When there's a pre-trained model available that has been trained on a large dataset related to your task (e.g., BERT for NLP, ResNet/ViT for computer vision).</li>
                <li>It's the standard approach for leveraging large foundation models (LLMs, large vision models).</li>
            </ul>
            <h4>How to Use (Coding Instruction - One Liner):</h4>
            <div class="code-block"><pre><code class="language-python">
# PyTorch: Load a pre-trained model and set up optimizer
# model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=num_classes)
# optimizer = AdamW(model.parameters(), lr=2e-5)
# model.train() # Set model to training mode
            </code></pre></div>
            <h4>Different Types / Variations:</h4>
            <ul>
                <li><strong>Full Fine-tuning:</strong> All layers of the pre-trained model are unfrozen and trained (updated) on the new task-specific data.
                    <ul>
                        <li><em>Pros:</em> Potentially highest performance, as the entire model adapts.</li>
                        <li><em>Cons:</em> Computationally expensive, requires more memory, more prone to catastrophic forgetting if the new task is very different or data is small.</li>
                        <li><em>When to Use:</em> When you have sufficient task-specific data, and the task is reasonably related to the pre-training task.</li>
                    </ul>
                </li>
                <li><strong>Feature Extraction (Frozen Layers):</strong> Only the final classification/regression head (a few top layers) of the pre-trained model is trained on the new data, while the pre-trained backbone layers are frozen (their weights are not updated).
                    <ul>
                        <li><em>Pros:</em> Computationally cheap, very fast, memory efficient, less prone to overfitting on small datasets.</li>
                        <li><em>Cons:</em> May not achieve optimal performance if the pre-trained features aren't perfectly aligned with the new task.</li>
                        <li><em>When to Use:</em> When task-specific data is very limited, or computational resources are constrained. Often used as a quick baseline.</li>
                    </ul>
                </li>
                <li><strong>Parameter-Efficient Fine-Tuning (PEFT):</strong> A family of techniques designed to update only a small fraction of the model's parameters during fine-tuning, while keeping most of the pre-trained weights frozen.
                    <ul>
                        <li><strong>LoRA (Low-Rank Adaptation):</strong> Inserts small, trainable low-rank matrices into the Transformer layers.</li>
                        <li><strong>Prefix-tuning / Prompt-tuning:</strong> Learns a small number of continuous "prefix" or "prompt" tokens that are prepended to the input embeddings, without modifying the model's core weights.</li>
                        <li><strong>Adapter Layers:</strong> Inserts small, trainable neural network modules between layers of the frozen pre-trained model.</li>
                        <li><em>Pros:</em> Drastically reduces trainable parameters (often <1%), leading to much smaller memory footprint for training and storing multiple fine-tuned versions, faster training, less catastrophic forgetting.</li>
                        <li><em>Cons:</em> Might not always match the performance of full fine-tuning.</li>
                        <li><em>When to Use:</em> Ideal for fine-tuning very large LLMs where full fine-tuning is prohibitively expensive, or when you need to fine-tune one model for many different downstream tasks.</li>
                    </ul>
                </li>
            </ul>
            <h4>When to Use What Type:</h4>
            <ul>
                <li>For large language models (LLMs) and when deploying many task-specific models from one base model, <strong>PEFT methods (especially LoRA)</strong> are the de-facto standard due to their efficiency.</li>
                <li>For smaller deep learning models or when you have abundant task-specific data, <strong>full fine-tuning</strong> might still yield the best results if computational budget allows.</li>
                <li><strong>Feature extraction</strong> is a good quick baseline or for very restrictive resource environments/data scarcity.</li>
            </ul>
            <h4>Coding Related to That (Detailed):</h4>
            <div class="code-block"><pre><code class="language-python">
from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset
import numpy as np
import evaluate
import torch

# --- 1. Setup: Load a small dataset and tokenizer ---
# This example uses a simplified text classification task.
dataset = load_dataset("imdb") # A larger dataset, will use subset for quick demo
# Limit to smaller subset for faster demo
small_train_dataset = dataset["train"].shuffle(seed=42).select(range(500))
small_eval_dataset = dataset["test"].shuffle(seed=42).select(range(100))

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_train_dataset = small_train_dataset.map(tokenize_function, batched=True)
tokenized_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)

# Rename label column to 'labels' as required by Trainer
tokenized_train_dataset = tokenized_train_dataset.remove_columns(["text"]).rename_column("label", "labels")
tokenized_eval_dataset = tokenized_eval_dataset.remove_columns(["text"]).rename_column("label", "labels")
tokenized_train_dataset.set_format("torch")
tokenized_eval_dataset.set_format("torch")

# --- 2. Full Fine-tuning Example ---
print("\n--- Full Fine-tuning Example (Conceptual) ---")
# Load a pre-trained BERT model with a classification head
model_full_ft = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2) # 2 classes: positive/negative

# Define metrics
metric = evaluate.load("accuracy")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# Configure training arguments
training_args_full = TrainingArguments(
    output_dir="./results_full_ft",
    num_train_epochs=1, # Reduced for demo
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=50,
    weight_decay=0.01,
    logging_dir="./logs_full_ft",
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    report_to="none" # Disable logging to external services for clean demo
)

# Create a Trainer for fine-tuning
trainer_full_ft = Trainer(
    model=model_full_ft,
    args=training_args_full,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_eval_dataset,
    compute_metrics=compute_metrics,
)

# Start fine-tuning
# trainer_full_ft.train() # Uncomment to run actual training

print("Full fine-tuning setup complete. (Training step commented out for brevity.)")


# --- 3. Feature Extraction Example (Conceptual) ---
print("\n--- Feature Extraction Example (Conceptual) ---")
# Load a pre-trained BERT model
model_feature_extraction = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# Freeze the backbone layers (all layers except the classification head)
for param in model_feature_extraction.bert.parameters():
    param.requires_grad = False

# Now only the classification head parameters (model_feature_extraction.classifier) are trainable
# You would then train this model similar to the full fine-tuning example.

# Verify trainable parameters
total_params = sum(p.numel() for p in model_feature_extraction.parameters())
trainable_params = sum(p.numel() for p in model_feature_extraction.parameters() if p.requires_grad)

print(f"Total parameters (Feature Extraction): {total_params}")
print(f"Trainable parameters (Feature Extraction): {trainable_params} ({trainable_params/total_params*100:.2f}%)")
print("Feature extraction setup complete. (Training step commented out for brevity.)")

# --- 4. Parameter-Efficient Fine-Tuning (PEFT - LoRA) Example (Conceptual) ---
print("\n--- PEFT (LoRA) Example (Conceptual) ---")
from peft import LoraConfig, get_peft_model, TaskType

# Load the base model
model_lora = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# Define LoRA config
lora_config = LoraConfig(
    task_type=TaskType.SEQ_CLS, # Sequence Classification
    inference_mode=False,
    r=8, # Rank of the low-rank matrices
    lora_alpha=16, # Scaling factor for LoRA
    lora_dropout=0.1,
    target_modules=["query", "value"], # Modules to apply LoRA to (typically Q and V in attention)
)

# Get the PEFT model
model_lora = get_peft_model(model_lora, lora_config)
model_lora.print_trainable_parameters() # Prints number of trainable parameters for LoRA

# This `model_lora` can then be trained using the Trainer as in full fine-tuning,
# but only the LoRA adapters will be updated.

print("PEFT (LoRA) setup complete. (Training step commented out for brevity.)")
            </code></pre></div>
        </div>

        <div class="concept-section">
            <h3>11. Quantization</h3>
            <h4>Concept Definition:</h4>
            <p>
                <strong>Quantization</strong> in machine learning is the process of reducing the precision of the numerical representations used for model parameters (weights, biases) and activations. For example, converting 32-bit floating-point numbers (FP32) to lower-precision formats like 16-bit floating-point (FP16), 8-bit integers (INT8), or even lower.
            </p>
            <h4>Why Used:</h4>
            <ul>
                <li><strong>Reduced Model Size:</strong> Lower precision numbers take up less memory, making models smaller and easier to store, especially on edge devices.</li>
                <li><strong>Faster Inference:</strong> Operations on lower-precision numbers are typically faster and consume less computational power (FLOPS) on specialized hardware (e.g., mobile CPUs, TPUs, GPUs with INT8 support).</li>
                <li><strong>Lower Energy Consumption:</strong> Less data movement and simpler computations lead to reduced energy usage, critical for battery-powered devices.</li>
            </ul>
            <h4>When to Use:</h4>
            <ul>
                <li>Primarily during the deployment phase of models (inference).</li>
                <li>When deploying models to resource-constrained environments (mobile phones, IoT devices, embedded systems).</li>
                <li>To speed up inference on server-side GPUs or CPUs.</li>
                <li>For large language models (LLMs) to fit them into memory or run them faster on consumer hardware.</li>
            </ul>
            <h4>How to Use (Coding Instruction - One Liner):</h4>
            <div class="code-block"><pre><code class="language-python">
# PyTorch: Dynamic quantization for a linear layer
# quantized_model = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)

# TensorFlow Lite: Convert a Keras model to a quantized TFLite model
# converter = tf.lite.TFLiteConverter.from_keras_model(model)
# converter.optimizations = [tf.lite.Optimize.DEFAULT]
# tflite_quant_model = converter.convert()
            </code></pre></div>
            <h4>Different Types / Variations:</h4>
            <ul>
                <li><strong>Post-training Quantization (PTQ):</strong> Quantization is applied to an already trained floating-point model. This is the simplest and most common approach.
                    <ul>
                        <li><strong>Dynamic Quantization:</strong> Weights are quantized ahead of time (e.g., to INT8), while activations are quantized on the fly during inference. Minimal accuracy impact, easy to implement.</li>
                        <li><strong>Static Quantization (Calibration):</strong> Both weights and activations are quantized. Requires running a small representative dataset through the model *before* quantization to determine appropriate quantization ranges for activations. Offers more performance gains but can have higher accuracy drops if calibration data isn't representative.</li>
                    </ul>
                    <li><em>When to Use:</em> When you want a quick and easy way to reduce model size/speed up inference with minimal code changes. Dynamic for ease, Static for more control and potential speedup.</li>
                </li>
                <li><strong>Quantization-Aware Training (QAT):</strong> Quantization effects are simulated during the training process itself. The model learns to be robust to quantization noise.
                    <ul>
                        <li><em>Pros:</em> Generally achieves higher accuracy than PTQ for the same bit width, as the model "learns" to deal with the lower precision.</li>
                        <li><em>Cons:</em> More complex to implement, requires retraining the model, can increase training time.</li>
                        <li><em>When to Use:</em> When achieving maximum possible accuracy at a given low bit width (e.g., INT8) is critical, even at the cost of more development effort and training time.</li>
                    </ul>
                </li>
                <li><strong>Different Bit Widths:</strong> Common quantization targets include INT8, INT4, FP16 (float16).
                    <ul>
                        <li>FP16 is often used during training as well as inference, offering memory savings with minimal accuracy loss.</li>
                        <li>INT8 is the most common for deploying truly "quantized" models for efficiency.</li>
                        <li>INT4/2 is experimental but gaining traction for LLMs to fit massive models on consumer GPUs, though often with a noticeable accuracy trade-off.</li>
                    </ul>
                </li>
            </ul>
            <h4>When to Use What Type:</h4>
            <ul>
                <li><strong>For a quick and easy solution with acceptable accuracy, use Post-training Dynamic Quantization.</strong></li>
                <li><strong>For higher performance gains but with careful management of potential accuracy drops, use Post-training Static Quantization and calibrate carefully.</strong></li>
                <li><strong>When accuracy is paramount and you need the best possible performance at a given low bit-width, Quantization-Aware Training (QAT) is the way to go, but it requires more effort.</strong></li>
                <li><strong>For extremely large LLMs, even FP16 is often considered a form of "quantization" from FP32 for training and inference, and INT4/8 is used for further deployment.</strong></li>
            </ul>
            <h4>Coding Related to That (Detailed):</h4>
            <div class="code-block"><pre><code class="language-python">
import torch
import torch.nn as nn
import torch.quantization
import copy

# Define a simple model
class SimpleNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(10, 5)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(5, 1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Instantiate and "train" a dummy model (weights are random for demo)
model = SimpleNet()
# In a real scenario, you would train this model first
# For demo, let's just make sure it has some weights
_ = model(torch.randn(1, 10))

# --- 1. Model Size Comparison ---
def print_model_size(model, label="Model"):
    param_size = 0
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    buffer_size = 0
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()
    size_all_mb = (param_size + buffer_size) / 1024**2
    print(f"{label} size: {size_all_mb:.2f} MB")

print_model_size(model, "Original FP32 Model")

# --- 2. Post-training Dynamic Quantization (PyTorch) ---
# Quantize only Linear layers to qint8
quantized_model_dynamic = torch.quantization.quantize_dynamic(
    model,      # the original model
    {nn.Linear}, # a set of layers to dynamically quantize
    dtype=torch.qint8 # the target dtype for quantized weights
)

print_model_size(quantized_model_dynamic, "Dynamic Quantized Model")

# Example inference with quantized model
dummy_input = torch.randn(1, 10)
output_fp32 = model(dummy_input)
output_quant = quantized_model_dynamic(dummy_input)

print(f"FP32 Output: {output_fp32.item():.4f}")
print(f"Quantized Output (Dynamic): {output_quant.item():.4f}")

# --- 3. Post-training Static Quantization (Conceptual Setup) ---
# More involved, requires fusing modules and preparing model for calibration
print("\n--- Static Quantization Setup (Conceptual) ---")
model_static = copy.deepcopy(model)
model_static.eval() # Set model to evaluation mode

# Step 1: Fuse modules (e.g., Conv-ReLU, Linear-ReLU) for better quantization
# This improves accuracy by treating fused operations as single unit
# torch.quantization.fuse_modules(model_static, [['fc1', 'relu']], inplace=True)

# Step 2: Insert Quant/DeQuant stubs and observers
model_static.qconfig = torch.quantization.get_default_qconfig('fbgemm') # Or 'qnnpack' for ARM
torch.quantization.prepare(model_static, inplace=True)

# Step 3: Calibrate the model by running it on a representative dataset
# (Pseudo-code for calibration)
# with torch.no_grad():
#     for inputs, _ in calibration_loader:
#         model_static(inputs)

# Step 4: Convert the model to static quantized version
# torch.quantization.convert(model_static, inplace=True)
print("Static quantization setup involves module fusing, preparing, calibration, and conversion.")
print("Steps 1-4 are commented out as they require a full training/calibration loop.")

# --- 4. Quantization-Aware Training (QAT) (Conceptual Setup) ---
print("\n--- Quantization-Aware Training (QAT) Setup (Conceptual) ---")
model_qat = copy.deepcopy(model)
model_qat.train() # Set model to training mode

# QAT also requires fusing and preparing, then training with QAT specific qconfig
# model_qat.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
# torch.quantization.prepare_qat(model_qat, inplace=True)

# The model would then be trained normally, but the quantization ops are simulated.
# After training, it's converted to a truly quantized model.
# torch.quantization.convert(model_qat.eval(), inplace=True)
print("QAT involves preparing a model for QAT, training it, then converting it.")
print("Steps are commented out as they require a full training loop.")
            </code></pre></div>
        </div>

        <div class="concept-section">
            <h3>12. Optimizer</h3>
            <h4>Concept Definition:</h4>
            <p>
                An <strong>optimizer</strong> is an algorithm or function that adjusts the attributes of the neural network, such as weights and learning rate, to minimize the loss function. It drives the training process by iteratively updating the model's parameters based on the gradients computed from the loss.
            </p>
            <h4>Why Used:</h4>
            <ul>
                <li><strong>Efficient Learning:</strong> Optimizers efficiently navigate the complex, high-dimensional loss landscape to find optimal model parameters, preventing issues like getting stuck in local minima or slow convergence.</li>
                <li><strong>Generalization:</strong> A good optimizer not only minimizes training loss but also helps the model generalize well to unseen data.</li>
            </ul>
            <h4>When to Use:</h4>
            <ul>
                <li>In every training process of a deep learning model or any iterative machine learning model that learns parameters from data.</li>
            </ul>
            <h4>How to Use (Coding Instruction - One Liner):</h4>
            <div class="code-block"><pre><code class="language-python">
# PyTorch: Using Adam optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001)

# TensorFlow/Keras: Using Adam optimizer
# optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
            </code></pre></div>
            <h4>Different Types / Variations:</h4>
            <p>Optimizers can be broadly categorized:</p>
            <ul>
                <li><strong>Gradient Descent Variants:</strong>
                    <ul>
                        <li><strong>Stochastic Gradient Descent (SGD):</strong> Simple, but effective. Can include momentum (accelerates convergence in relevant direction) and Nesterov momentum (look-ahead).</li>
                        <li><em>When to Use:</em> Often performs well in terms of generalization but can be slow and sensitive to learning rate tuning. Good for robust models.</li>
                    </ul>
                </li>
                <li><strong>Adaptive Learning Rate Optimizers:</strong> Automatically adjust the learning rate during training for each parameter individually. These are often the default choice for deep learning.
                    <ul>
                        <li><strong>Adagrad (Adaptive Gradient Algorithm):</strong> Adapts learning rate to parameters, making larger updates for sparse features and smaller updates for frequent features.
                            $$ w_{t+1} = w_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \odot g_t $$
                            (where $G_t$ is the sum of squares of past gradients for each parameter)
                            <em>When to Use:</em> Effective for sparse data (e.g., NLP with rare words). Can lead to rapidly diminishing learning rates.</li>
                        <li><strong>RMSprop (Root Mean Square Propagation):</strong> Adapts learning rates based on the root mean square of recent gradients. Addresses Adagrad's rapidly diminishing LR.
                            $$ v_t = \gamma v_{t-1} + (1 - \gamma) g_t^2 $$
                            $$ w_{t+1} = w_t - \frac{\eta}{\sqrt{v_t + \epsilon}} \odot g_t $$
                            <em>When to Use:</em> Good general-purpose optimizer, often used for recurrent neural networks (RNNs).</li>
                        <li><strong>Adam (Adaptive Moments Estimation):</strong> Combines ideas from RMSprop and momentum. Computes adaptive learning rates for each parameter, and also keeps track of exponentially decaying average of past gradients and squared gradients.
                            <em>When to Use:</em> The most popular and generally recommended default optimizer for a wide range of deep learning tasks due to its robustness and efficiency.</li>
                        <li><strong>Adadelta:</strong> An extension of Adagrad that aims to reduce the aggressive, monotonically decreasing learning rate. Does not require a manually set learning rate.</li>
                        <li><strong>Nadam (Nesterov-accelerated Adaptive Moment Estimation):</strong> Adam with Nesterov momentum. Often slightly better than Adam.</li>
                    </ul>
                </li>
                <li><strong>Second-Order Optimizers (Rare in Deep Learning):</strong>
                    <ul>
                        <li><strong>L-BFGS:</strong> Approximates the Hessian matrix. More computationally expensive per step but can converge in fewer steps.
                            <em>When to Use:</em> Often used for convex optimization problems or small neural networks, not common for large deep learning models due to high computational cost.</li>
                    </ul>
                </li>
            </ul>
            <h4>When to Use What Type:</h4>
            <ul>
                <li><strong>Adam:</strong> Generally the <strong>default recommendation</strong> for most deep learning models due to its good performance, robustness, and ease of use (less hyperparameter tuning).</li>
                <li><strong>RMSprop:</strong> A strong alternative to Adam, particularly good for RNNs.</li>
                <li><strong>SGD with Momentum (or Nesterov):</strong> Can sometimes achieve better generalization than adaptive optimizers if carefully tuned, but requires more manual learning rate scheduling. Often preferred for state-of-the-art results in specific domains like computer vision.</li>
                <li><strong>Adagrad:</strong> When dealing with very sparse data.</li>
                <li>Avoid L-BFGS for typical large deep learning models.</li>
            </ul>
            <h4>Coding Related to That (Detailed):</h4>
            <div class="code-block"><pre><code class="language-python">
import torch
import torch.nn as nn
import torch.optim as optim

class SimpleNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(10, 1)
    def forward(self, x):
        return self.linear(x)

model = SimpleNet()
criterion = nn.MSELoss()
inputs = torch.randn(32, 10)
targets = torch.randn(32, 1)

# --- 1. Stochastic Gradient Descent (SGD) ---
optimizer_sgd = optim.SGD(model.parameters(), lr=0.01)
# You can add momentum: optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
# And Nesterov momentum: optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)
optimizer_sgd.zero_grad()
loss_sgd = criterion(model(inputs), targets)
loss_sgd.backward()
optimizer_sgd.step()
print(f"SGD Loss after one step: {loss_sgd.item():.4f}")

# --- 2. Adam Optimizer ---
# Adam is generally a great default choice
optimizer_adam = optim.Adam(model.parameters(), lr=0.001)
optimizer_adam.zero_grad()
loss_adam = criterion(model(inputs), targets)
loss_adam.backward()
optimizer_adam.step()
print(f"Adam Loss after one step: {loss_adam.item():.4f}")

# --- 3. RMSprop Optimizer ---
optimizer_rmsprop = optim.RMSprop(model.parameters(), lr=0.001)
optimizer_rmsprop.zero_grad()
loss_rmsprop = criterion(model(inputs), targets)
loss_rmsprop.backward()
optimizer_rmsprop.step()
print(f"RMSprop Loss after one step: {loss_rmsprop.item():.4f}")

# --- 4. Adagrad Optimizer ---
optimizer_adagrad = optim.Adagrad(model.parameters(), lr=0.01)
optimizer_adagrad.zero_grad()
loss_adagrad = criterion(model(inputs), targets)
loss_adagrad.backward()
optimizer_adagrad.step()
print(f"Adagrad Loss after one step: {loss_adagrad.item():.4f}")

# --- General training loop structure ---
# for epoch in range(num_epochs):
#     for batch_inputs, batch_targets in data_loader:
#         optimizer.zero_grad()
#         predictions = model(batch_inputs)
#         loss = criterion(predictions, batch_targets)
#         loss.backward()
#         optimizer.step()
            </code></pre></div>
        </div>

        <div class="concept-section">
            <h3>13. Pre-training</h3>
            <h4>Concept Definition:</h4>
            <p>
                <strong>Pre-training</strong> is the process of training a machine learning model (typically a deep neural network) on a very large dataset to learn general, transferable features or representations. This initial training phase is often performed on a broad task (e.g., predicting the next word in a sentence, classifying millions of images) that does not require specific labels.
            </p>
            <h4>Why Used:</h4>
            <ul>
                <li><strong>Feature Learning:</strong> Enables models to learn rich, hierarchical features from massive amounts of data that can be useful for a wide range of downstream tasks.</li>
                <li><strong>Knowledge Transfer (Transfer Learning):</strong> The learned representations act as a strong initialization for specific tasks, drastically reducing the need for large, labeled datasets for those tasks.</li>
                <li><strong>Reduced Training Time & Resources:</strong> Fine-tuning a pre-trained model is far more efficient (faster, less data) than training a complex model from scratch.</li>
                <li><strong>State-of-the-Art Performance:</strong> Pre-training, followed by fine-tuning, is the dominant paradigm for achieving top performance in many domains like NLP and Computer Vision.</li>
            </ul>
            <h4>When to Use:</h4>
            <ul>
                <li>When starting any complex deep learning project, especially if you don't have extremely large labeled datasets for your specific task.</li>
                <li>When working with natural language (e.g., text classification, summarization, question answering) or images (e.g., object detection, image classification).</li>
                <li>Whenever a suitable pre-trained model exists for your domain.</li>
            </ul>
            <h4>How to Use (Coding Instruction - One Liner):</h4>
            <div class="code-block"><pre><code class="language-python">
# PyTorch/Hugging Face Transformers: Load a pre-trained model
from transformers import AutoModel
model = AutoModel.from_pretrained("bert-base-uncased")

# TensorFlow/Keras: Load pre-trained weights
# model = tf.keras.applications.ResNet50(weights='imagenet')
            </code></pre></div>
            <h4>Different Types / Variations:</h4>
            <ul>
                <li><strong>Supervised Pre-training:</strong>
                    <ul>
                        <li>Model is trained on a large, labeled dataset for a supervised task (e.g., ImageNet for image classification). The learned features are then transferred.</li>
                        <li><em>Examples:</em> ImageNet pre-trained models (ResNet, VGG, EfficientNet).</li>
                        <li><em>When to Use:</em> When a large, diverse, labeled dataset for a general task is available (e.g., general object recognition).</li>
                    </ul>
                </li>
                <li><strong>Unsupervised / Self-supervised Pre-training:</strong>
                    <ul>
                        <li>Model learns representations from unlabeled data by setting up a "pretext task" where the labels are automatically generated from the input data itself.</li>
                        <li><strong>Masked Language Modeling (MLM):</strong> (e.g., BERT) Predict masked words in a sentence.</li>
                        <li><strong>Next Sentence Prediction (NSP):</strong> (e.g., BERT) Predict if two sentences follow each other.</li>
                        <li><strong>Causal Language Modeling (CLM):</strong> (e.g., GPT) Predict the next word in a sequence.</li>
                        <li><strong>Denoising Autoencoders:</strong> Reconstruct original input from a corrupted version.</li>
                        <li><strong>Contrastive Learning:</strong> (e.g., SimCLR, CLIP) Learn representations by pushing similar samples closer and dissimilar samples further apart in embedding space.</li>
                    </ul>
                    <li><em>When to Use:</em> Dominant for NLP (due to abundance of unlabeled text) and increasingly popular in Computer Vision for learning robust representations without human labels.</li>
                </li>
            </ul>
            <h4>When to Use What Type:</h4>
            <ul>
                <li>For <strong>Natural Language Processing (NLP)</strong>, almost exclusively use models pre-trained with <strong>self-supervised learning objectives (MLM, CLM)</strong> on massive text corpora (e.g., BERT, GPT, Llama, T5).</li>
                <li>For <strong>Computer Vision (CV)</strong>, you can use models pre-trained with <strong>supervised learning on ImageNet</strong> (e.g., ResNet, EfficientNet) or increasingly, models pre-trained with <strong>self-supervised or contrastive learning</strong> (e.g., SimCLR, MAE, DINO). Transformers are also being pre-trained on large image datasets.</li>
                <li>The choice depends on the availability of pre-trained models relevant to your domain and the nature of their pre-training task.</li>
            </ul>
            <h4>Coding Related to That (Detailed):</h4>
            <div class="code-block"><pre><code class="language-python">
from transformers import AutoModel, AutoTokenizer
from torchvision import models # For vision models
import torch

# --- 1. Pre-training for NLP (using Hugging Face Transformers) ---
# Load a pre-trained BERT model (encoder-only) and its tokenizer
nlp_model_name = "bert-base-uncased"
tokenizer_nlp = AutoTokenizer.from_pretrained(nlp_model_name)
model_nlp = AutoModel.from_pretrained(nlp_model_name)

print(f"\n--- NLP Pre-trained Model ({nlp_model_name}) ---")
print(f"Model class: {type(model_nlp)}")
print(f"Example: Tokenizing and getting embeddings for a sentence.")
input_text = "Pre-training helps models learn general knowledge."
inputs = tokenizer_nlp(input_text, return_tensors="pt")
with torch.no_grad():
    outputs = model_nlp(**inputs)
# The last_hidden_state are the contextual embeddings learned by BERT
print(f"Last hidden state shape (contextual embeddings): {outputs.last_hidden_state.shape}")


# --- 2. Pre-training for Computer Vision (using torchvision) ---
# Load a pre-trained ResNet-50 model on ImageNet
vision_model = models.resnet50(weights='IMAGENET1K_V1') # Loads pre-trained weights
vision_model.eval() # Set to evaluation mode if not training further

print(f"\n--- Vision Pre-trained Model (ResNet-50 on ImageNet) ---")
print(f"Model class: {type(vision_model)}")
# Example: Dummy input for an image (batch_size, channels, height, width)
dummy_image_input = torch.randn(1, 3, 224, 224)
with torch.no_grad():
    features = vision_model(dummy_image_input)
print(f"Output features shape (after backbone): {features.shape}") # Typically 1000 for ImageNet output

# Note: After loading, these pre-trained models would typically be followed by
# adding a task-specific head (e.g., a linear layer for your specific classification task)
# and then fine-tuned on your dataset.
            </code></pre></div>
        </div>

        <div class="concept-section">
            <h3>14. Regularization</h3>
            <h4>Concept Definition:</h4>
            <p>
                <strong>Regularization</strong> refers to a set of techniques used in machine learning to prevent overfitting. Overfitting occurs when a model learns the training data too well, including its noise and specific patterns, leading to poor generalization performance on unseen data. Regularization methods introduce a penalty or constraint during training to discourage overly complex models.
            </p>
            <h4>Why Used:</h4>
            <ul>
                <li><strong>Prevent Overfitting:</strong> The primary goal is to improve the model's ability to generalize to new, unseen data by reducing its complexity and sensitivity to noise in the training set.</li>
                <li><strong>Bias-Variance Trade-off:</strong> Regularization helps strike a better balance in the bias-variance trade-off, typically by increasing bias slightly to significantly reduce variance.</li>
                <li><strong>Stabilize Training:</strong> Some regularization techniques (like Batch Normalization) can also help stabilize and accelerate training.</li>
            </ul>
            <h4>When to Use:</h4>
            <ul>
                <li>Almost always necessary when training complex models, especially deep neural networks, which have a very high capacity and are prone to overfitting.</li>
                <li>When you observe a significant gap between training accuracy/loss and validation accuracy/loss (a sign of overfitting).</li>
            </ul>
            <h4>How to Use (Coding Instruction - One Liner):</h4>
            <div class="code-block"><pre><code class="language-python">
# L2 Regularization (Weight Decay) in optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)

# Dropout layer in a neural network
# self.dropout = nn.Dropout(p=0.5)
# x = self.dropout(x)
            </code></pre></div>
            <h4>Different Types / Variations:</h4>
            <ul>
                <li><strong>L1 Regularization (Lasso Regularization):</strong> Adds the absolute value of the magnitude of coefficients as a penalty to the loss function. Encourages sparsity (some coefficients become exactly zero), effectively performing feature selection.
                    $$ \text{Loss}_{L1} = \text{Loss} + \lambda \sum_{j=1}^{M} |w_j| $$</li>
                <li><strong>L2 Regularization (Ridge Regularization / Weight Decay):</strong> Adds the squared magnitude of coefficients as a penalty. It encourages weights to be small but rarely exactly zero.
                    $$ \text{Loss}_{L2} = \text{Loss} + \lambda \sum_{j=1}^{M} w_j^2 $$
                    This is often implemented as `weight_decay` in optimizers.</li>
                <li><strong>Dropout:</strong> Randomly sets a fraction of neuron outputs to zero during each training iteration. This forces the network to learn more robust features that don't rely on the presence of any single neuron.
                    <ul>
                        <li><em>When to Use:</em> Highly effective for neural networks. Apply after activation functions or between layers.</li>
                    </ul>
                </li>
                <li><strong>Early Stopping:</strong> Monitor the model's performance (e.g., validation loss) on a separate validation set during training. Stop training when the performance on the validation set starts to degrade, even if the training loss is still decreasing.
                    <ul>
                        <li><em>When to Use:</em> A simple yet very effective general regularization technique applicable to almost any iterative model.</li>
                    </ul>
                </li>
                <li><strong>Data Augmentation:</strong> Artificially expands the training dataset by creating modified versions of existing data (e.g., for images: rotation, flipping, cropping; for text: synonym replacement, back-translation).
                    <ul>
                        <li><em>When to Use:</em> Particularly powerful for computer vision and increasingly for NLP.</li>
                    </ul>
                </li>
                <li><strong>Batch Normalization:</strong> Normalizes the inputs to a layer for each mini-batch. It helps to stabilize and speed up training, and also acts as a mild regularizer by adding some noise.
                    <ul>
                        <li><em>When to Use:</em> Commonly inserted between linear/convolutional layers and their activation functions in deep neural networks.</li>
                    </ul>
                </li>
                <li><strong>Ensemble Methods:</strong> (e.g., Random Forests, Bagging, Boosting) While not explicitly a regularization technique applied to a single model, combining multiple models implicitly reduces variance and overfitting.</li>
            </ul>
            <h4>When to Use What Type:</h4>
            <ul>
                <li><strong>L1/L2 Regularization:</strong> Use L2 (weight decay) as a general default for most neural networks. L1 can be useful if you suspect many features are irrelevant and want automatic feature selection.</li>
                <li><strong>Dropout:</strong> Almost always use in deep neural networks (especially fully connected layers). The dropout rate (p) is a hyperparameter to tune (e.g., 0.2 to 0.5).</li>
                <li><strong>Early Stopping:</strong> Highly recommended for all deep learning training to prevent overfitting and save computational resources.</li>
                <li><strong>Data Augmentation:</strong> Essential for vision tasks and increasingly used in NLP, especially when the training dataset is limited.</li>
                <li><strong>Batch Normalization:</strong> Widely used in most modern deep neural network architectures, helps with training stability and implicitly regularizes.</li>
                <li><strong>Ensemble Methods:</strong> Often, a combination of these techniques is used (e.g., L2 + Dropout + Early Stopping + Data Augmentation).</li>
            </ul>
            <h4>Coding Related to That (Detailed):</h4>
            <div class="code-block"><pre><code class="language-python">
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms
from torch.utils.data import DataLoader, TensorDataset

# --- Simulate a simple model ---
class RegularizedNet(nn.Module):
    def __init__(self, dropout_rate=0.5):
        super().__init__()
        self.fc1 = nn.Linear(100, 50)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=dropout_rate) # Dropout layer
        self.fc2 = nn.Linear(50, 1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x) # Apply dropout
        x = self.fc2(x)
        return x

# Simulate dummy data (more features to show regularization effect)
X_train = torch.randn(100, 100) # Small dataset, high features -> prone to overfitting
y_train = torch.randn(100, 1)
X_val = torch.randn(20, 100)
y_val = torch.randn(20, 1)

train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=16)
val_dataset = TensorDataset(X_val, y_val)
val_loader = DataLoader(val_dataset, batch_size=16)


# --- 1. L2 Regularization (Weight Decay) & Dropout ---
print("\n--- L2 Regularization (Weight Decay) & Dropout Example ---")
model_reg = RegularizedNet(dropout_rate=0.5) # Instantiate model with Dropout
criterion = nn.MSELoss()
# L2 regularization is added via the 'weight_decay' parameter in the optimizer
optimizer_reg = optim.Adam(model_reg.parameters(), lr=0.01, weight_decay=0.001) # L2 penalty

num_epochs = 20
best_val_loss = float('inf')
patience = 5
patience_counter = 0

for epoch in range(num_epochs):
    # Training phase
    model_reg.train() # Set to training mode (dropout active)
    train_loss = 0
    for inputs, targets in train_loader:
        optimizer_reg.zero_grad()
        outputs = model_reg(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer_reg.step()
        train_loss += loss.item()

    # Validation phase
    model_reg.eval() # Set to evaluation mode (dropout inactive)
    val_loss = 0
    with torch.no_grad():
        for inputs_val, targets_val in val_loader:
            outputs_val = model_reg(inputs_val)
            loss_val = criterion(outputs_val, targets_val)
            val_loss += loss_val.item()

    avg_train_loss = train_loss / len(train_loader)
    avg_val_loss = val_loss / len(val_loader)
    print(f"Epoch {epoch+1:2d}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")

    # --- Early Stopping (integrated into loop) ---
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        patience_counter = 0 # Reset patience
        # Save best model here: torch.save(model_reg.state_dict(), 'best_model.pth')
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print(f"Early stopping triggered at Epoch {epoch+1} due to no improvement for {patience} epochs.")
            break

# --- 2. Data Augmentation (for images - conceptual) ---
print("\n--- Data Augmentation (Conceptual for Images) ---")
# from torchvision import datasets, transforms
# Define a transform pipeline with augmentation
# transform_augment = transforms.Compose([
#     transforms.RandomResizedCrop(224),
#     transforms.RandomHorizontalFlip(),
#     transforms.ColorJitter(brightness=0.2, contrast=0.2),
#     transforms.ToTensor(),
#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
# ])
# train_dataset_aug = datasets.ImageFolder('path/to/train_data', transform=transform_augment)
# train_loader_aug = DataLoader(train_dataset_aug, batch_size=32, shuffle=True)
print("Data Augmentation is applied as part of the data loading/preprocessing pipeline.")
print("Example transforms include RandomResizedCrop, RandomHorizontalFlip, ColorJitter etc.")

# --- 3. Batch Normalization (conceptual) ---
print("\n--- Batch Normalization (Conceptual) ---")
class BatchNormNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(100, 50)
        self.bn1 = nn.BatchNorm1d(50) # Batch Normalization layer
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(50, 1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.bn1(x) # Apply Batch Normalization
        x = self.relu(x)
        x = self.fc2(x)
        return x
# Batch Normalization layers are added directly into the model architecture.
print("Batch Normalization layers are integrated directly into the neural network architecture.")
print("They normalize activations within each batch during training.")
            </code></pre></div>
        </div>

        <div class="concept-section">
            <h3>15. Perplexity</h3>
            <h4>Concept Definition:</h4>
            <p>
                <strong>Perplexity</strong> is a common evaluation metric for language models. It measures how well a probability distribution or language model predicts a sample. In simpler terms, it's a measure of how "surprised" the model is by new data. A lower perplexity score indicates a better language model.
            </p>
            <p>
                Mathematically, perplexity (PP) is defined as 2 raised to the power of the cross-entropy loss (or negative log-likelihood) of the model on the test set, averaged per token.
                $$ \text{Perplexity}(P, Q) = 2^{H(P, Q)} = 2^{-\frac{1}{N} \sum_{i=1}^{N} \log_2 P(\text{word}_i | \text{context}_i)} $$
                where $H(P, Q)$ is the cross-entropy between the true distribution $P$ and the predicted distribution $Q$, and $N$ is the total number of tokens. A lower value means the model is more confident and accurate in its predictions.
            </p>
            <h4>Why Used:</h4>
            <ul>
                <li><strong>Language Model Evaluation:</strong> It's the standard intrinsic evaluation metric for generative language models (e.g., n-gram models, LSTMs, Transformers).</li>
                <li><strong>Comparable:</strong> Allows for quantitative comparison between different language models on the same dataset.</li>
                <li><strong>Intuitive Meaning (Inverse Branching Factor):</strong> Can be loosely interpreted as the "weighted average number of choices" a model has when predicting the next word. For example, a perplexity of 10 means the model, on average, is as confused as if it had to choose uniformly among 10 words at each step.</li>
            </ul>
            <h4>When to Use:</h4>
            <ul>
                <li>When evaluating the performance of any type of language model, especially those trained for generative tasks (text generation, machine translation, speech recognition).</li>
                <li>To compare different model architectures or hyperparameter settings for language modeling.</li>
            </ul>
            <h4>How to Use (Coding Instruction - One Liner):</h4>
            <div class="code-block"><pre><code class="language-python">
# From a calculated cross-entropy loss
perplexity = torch.exp(cross_entropy_loss)

# Using Hugging Face evaluate library
# perplexity_metric = evaluate.load("perplexity")
# results = perplexity_metric.compute(model_id="gpt2", predictions=["hello world"])
            </code></pre></div>
            <h4>Different Types / Variations:</h4>
            <p>While the core formula remains, perplexity can be computed at different granularities or with specific normalizations:</p>
            <ul>
                <li><strong>Word-level Perplexity:</strong> Most common. The calculation is based on predicting individual words.</li>
                <li><strong>Character-level Perplexity:</strong> Less common, used for character-level language models. The sum is over characters, not words.</li>
                <li><strong>Bits Per Character/Word (BPC/BPW):</strong> A related metric, often used interchangeably, which is the negative log-likelihood divided by the number of characters/words. Perplexity is $2^{\text{BPC}}$ or $2^{\text{BPW}}$.</li>
                <li><strong>Perplexity of a Model vs. Perplexity of a Dataset:</strong> Often, when we say "perplexity," we mean the perplexity of a model on a given dataset.</li>
            </ul>
            <h4>When to Use What Type:</h4>
            <ul>
                <li><strong>Word-level perplexity</strong> is the standard for evaluating most modern language models (which are typically trained at the word or subword level).</li>
                <li><strong>Character-level perplexity</strong> is only relevant if your language model operates at the character level.</li>
                <li>The interpretation and comparison of perplexity scores should always be done on the same dataset and tokenization scheme. Different tokenizers (e.g., word-level vs. subword) will yield different perplexity values for the same underlying text due to varying numbers of tokens.</li>
            </ul>
            <h4>Coding Related to That (Detailed):</h4>
            <div class="code-block"><pre><code class="language-python">
import torch
import torch.nn.functional as F
import numpy as np

# --- 1. Calculating Perplexity from Cross-Entropy Loss ---
# Simulate ground truth labels (actual next word IDs)
# batch_size=2, sequence_length=3 (next 3 words)
true_next_word_ids = torch.tensor([
    [10, 25, 300], # For first sequence
    [5,  12,  50]  # For second sequence
])

# Simulate model's predicted logits for the next word for each position
# (batch_size, sequence_length, vocab_size)
vocab_size = 1000
predicted_logits = torch.randn(2, 3, vocab_size) # Random logits for demo

# Reshape logits and targets for CrossEntropyLoss
# CrossEntropyLoss expects (N, C) for input (logits) and (N) for target (class_indices)
# N = batch_size * sequence_length, C = vocab_size
logits_reshaped = predicted_logits.view(-1, vocab_size) # (6, 1000)
targets_reshaped = true_next_word_ids.view(-1) # (6)

# Calculate Cross-Entropy Loss
cross_entropy_loss_fn = nn.CrossEntropyLoss(reduction='mean') # 'mean' for average loss per token
loss = cross_entropy_loss_fn(logits_reshaped, targets_reshaped)

print("--- Perplexity Calculation Example ---")
print(f"Calculated Cross-Entropy Loss: {loss.item():.4f}")

# Convert loss to perplexity
perplexity = torch.exp(loss) # perplexity = e^(cross_entropy_loss)
# If using log base 2 for cross-entropy, then it's 2^loss. PyTorch uses natural log.
print(f"Perplexity: {perplexity.item():.4f}")

# --- 2. Using Hugging Face evaluate library (high-level) ---
# This often handles tokenization and model inference internally
# from evaluate import load
# perplexity_metric = load("perplexity", module_type="metric")

# # Example with a pre-trained model (actual model and text)
# # Requires model_id (e.g., "gpt2"), and predictions (list of strings or tokenized inputs)
# # results = perplexity_metric.compute(model_id="gpt2",
# #                                    add_start_token=False, # Often False for evaluation on text segments
# #                                    predictions=["The quick brown fox", "jumps over the lazy dog"])
# # print(f"\nHugging Face Perplexity Results: {results}")

print("\nHugging Face `evaluate` library provides a high-level way to compute perplexity.")
print("Example usage with 'evaluate.load(\"perplexity\")' is conceptual here.")
            </code></pre></div>
        </div>

        <h2>Further Learning Resources</h2>
        <p>To deepen your understanding of these core concepts, consider exploring the following resources:</p>
        <h3>Online Courses:</h3>
        <ul>
            <li><a href="https://www.coursera.org/specializations/deep-learning" target="_blank">Deep Learning Specialization (Andrew Ng, Coursera)</a> - Covers many of these concepts in detail.</li>
            <li><a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html" target="_blank">PyTorch Autograd Mechanics</a> - For deeper understanding of gradients and backpropagation in PyTorch.</li>
            <li><a href="https://web.stanford.edu/class/cs224n/index.html" target="_blank">Stanford CS224n: Natural Language Processing with Deep Learning</a> - Excellent for attention, tokens, context window, pre-training.</li>
        </ul>
        <h3>Key Textbooks:</h3>
        <ul>
            <li>"Deep Learning" by Ian Goodfellow, Yoshua Bengio, Aaron Courville. (The "Bible" of deep learning, covers mathematical foundations).</li>
            <li>"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow" by Aurlien Gron. (Practical, code-focused).</li>
            <li>"Neural Networks and Deep Learning" by Michael Nielsen. (Online book, great for fundamental concepts).</li>
        </ul>
        <h3>Simulated Attachments (Downloadable Resources):</h3>
        <ul>
            <li><a href="https://pending_domain.com/ml_dl_gradients_optimizers.pdf" download>Gradients and Optimizers Deep Dive (PDF)</a></li>
            <li><a href="https://pending_domain.com/llm_attention_tokens_context.pdf" download>Attention, Tokens, and Context in LLMs (PDF)</a></li>
            <li><a href="https://pending_domain.com/model_optimization_quant_regularization.pdf" download>Model Optimization, Quantization, and Regularization (PDF)</a></li>
        </ul>
    </div>

    <div class="footer">
        <p>&copy; 2025 ML Learning Guide - July, 2025. Designed for Computer Students & Programmers.</p>
    </div>

    <script>
        function loadPage(pageName) {
            window.location.href = pageName;
        }
    </script>
    <script type="text/javascript"
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML">
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            extensions: ["tex2jax.js"],
            jax: ["input/TeX", "output/HTML-CSS"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: true
            },
            "HTML-CSS": {
                availableFonts: ["TeX", "STIX", "NeoEuler", "AMS"],
                preferredFont: "TeX",
                imageFont: null,
                showMathMenu: true,
                scale: 100,
                linebreaks: { automatic: true },
                EqnChunk: { width: "80%", display: true }
            }
        });
    </script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/line-numbers/prism-line-numbers.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/line-numbers/prism-line-numbers.min.js"></script>
    <script>Prism.plugins.lineNumbers();</script>
    <script>Prism.highlightAll();</script>
</body>
</html>